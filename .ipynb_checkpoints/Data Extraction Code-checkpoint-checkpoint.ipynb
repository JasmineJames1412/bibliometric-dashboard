{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8e33682-f027-45fe-ad45-f8d16a7d83fa",
   "metadata": {},
   "source": [
    "# Data Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11426b1e-8710-4a5b-83c0-824997385463",
   "metadata": {},
   "source": [
    "## Author Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6095c282-2d6f-4c15-af3c-68ff4bef1faf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the author's name:  Rajendra Prasad\n",
      "Enter the author's affiliation (or keyword):  Professor Dean, Amity University Haryana\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for author profile at URL: https://scholar.google.com/scholar?q=Rajendra+Prasad+Professor+Dean,+Amity+University+Haryana&hl=en\n",
      "Found 'User profiles for' link: https://scholar.google.com/citations?view_op=search_authors&mauthors=Rajendra+Prasad+Professor+Dean,+Amity+University+Haryana&hl=en&oi=ao\n",
      "Redirected to Google sign-in page. Please run the script in non-headless mode, log in manually if prompted, or try from a different network.\n",
      "Page source for debugging (first 2000 characters):\n",
      "<html lang=\"en\" dir=\"ltr\" class=\"eC9N2e\"><head><base href=\"https://accounts.google.com/v3/signin/\"><link rel=\"preconnect\" href=\"//www.gstatic.com\"><meta name=\"referrer\" content=\"origin\"><script data-id=\"_gd\" nonce=\"\">window.WIZ_global_data = {\"DndLYb\":\"\",\"DpimGf\":false,\"EP1ykd\":[\"/_/*\"],\"EaOSAe\":\"https://kidsmanagement-pa.googleapis.com\",\"FdrFJe\":\"2457560614629463232\",\"FoW6je\":false,\"GWsdKe\":\"en\",\"HAZvpc\":\"https://accounts.google.com/v3/signin/interstitial/doritos/forward/success?continue\\u003dhttps://scholar.google.com/citations?view_op%3Dsearch_authors%26mauthors%3DRajendra%2BPrasad%2BProfessor%2BDean,%2BAmity%2BUniversity%2BHaryana%26hl%3Den%26oi%3Dao\\u0026timeStmp\\u003d1749626541067\",\"Im6cmf\":\"/v3/signin/_/AccountsSignInUi\",\"LVIXXb\":1,\"LoQv7e\":true,\"MT7f9b\":[],\"MUE6Ne\":\"AccountsSignInUi\",\"Mypbod\":\"https://www.googleapis.com/reauth\",\"QrtxK\":\"\",\"Qzxixc\":\"S1745722227:1749626540875513\",\"S06Grb\":\"\",\"S6lZl\":128566913,\"SNlM0e\":\"AEObw431WDybClL5jGir-IlRoUJR:1749626541069\",\"TSDtV\":\"%.@.[[null,[[45703546,null,null,null,\\\"\\\",null,\\\"eqLaD\\\"],[45450723,null,false,null,null,null,\\\"XQkE7e\\\"],[45453809,null,false,null,null,null,\\\"D1bn1b\\\"],[45451649,null,true,null,null,null,\\\"LeUU4b\\\"],[45651384,null,true,null,null,null,\\\"GmAFDc\\\"],[45682322,null,false,null,null,null,\\\"HgjJSe\\\"],[45451922,null,false,null,null,null,\\\"TFYsbf\\\"],[45647029,null,true,null,null,null,\\\"NpaWBe\\\"],[45667279,0,null,null,null,null,\\\"mIC16c\\\"],[45451657,null,true,null,null,null,\\\"XibpJd\\\"],[45459555,null,false,null,null,null,\\\"Imeoqb\\\"],[45699405,null,false,null,null,null,\\\"v7fSvc\\\"],[45686230,null,false,null,null,null,\\\"nfxin\\\"],[45684016,null,true,null,null,null,\\\"xsnBVe\\\"],[45690017,null,false,null,null,null,\\\"eSX4ub\\\"],[45686077,null,false,null,null,null,\\\"O5EhJf\\\"],[45692956,null,false,null,null,null,\\\"XurJcf\\\"],[45685953,null,true,null,null,null,\\\"BnD2Db\\\"],[45628954,null,true,null,null,null,\\\"ZjTJve\\\"],[45702488,null,false,null,null,null,\\\"YFj8kb\\\"],[45450722,null,true,null,null,null,\\\"b1Rcdc\\\"],[45\n",
      "Author not found. Check the name or affiliation keyword.\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def get_author_profile_url(author_name, author_affiliation):\n",
    "    options = Options()\n",
    "    # Run in non-headless mode to reduce blocking\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    # Set a user-agent to mimic a real browser\n",
    "    options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0.0.0 Safari/537.36')\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "    try:\n",
    "        # Construct the search URL\n",
    "        query = f\"{author_name} {author_affiliation}\".replace(\" \", \"+\")\n",
    "        url = f\"https://scholar.google.com/scholar?q={query}&hl=en\"\n",
    "        print(f\"Searching for author profile at URL: {url}\")\n",
    "        driver.get(url)\n",
    "        time.sleep(5)  # Wait for the page to load\n",
    "\n",
    "        # Scroll to ensure lazy-loaded content appears\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Check for CAPTCHA or block page\n",
    "        page_source = driver.page_source.lower()\n",
    "        if \"recaptcha\" in page_source or \"unusual traffic\" in page_source:\n",
    "            print(\"CAPTCHA or block page detected. Please try from a different network or wait a few hours.\")\n",
    "            driver.quit()\n",
    "            return None, None\n",
    "\n",
    "        # Find the \"User profiles for\" link\n",
    "        try:\n",
    "            profiles_link = WebDriverWait(driver, 60).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//a[contains(text(), 'User profiles for')]\"))\n",
    "            )\n",
    "            profiles_url = profiles_link.get_attribute(\"href\")\n",
    "            print(f\"Found 'User profiles for' link: {profiles_url}\")\n",
    "            profiles_link.click()\n",
    "            time.sleep(5)  # Increased wait for the profiles list page to load\n",
    "        except TimeoutException:\n",
    "            print(\"Timeout waiting for 'User profiles for' link.\")\n",
    "            print(\"Page source for debugging (first 2000 characters):\")\n",
    "            print(driver.page_source[:2000])\n",
    "            driver.quit()\n",
    "            return None, None\n",
    "\n",
    "        # Check for sign-in redirect\n",
    "        page_source = driver.page_source.lower()\n",
    "        if \"accounts.google.com\" in page_source or \"signin\" in page_source:\n",
    "            print(\"Redirected to Google sign-in page. Please run the script in non-headless mode, log in manually if prompted, or try from a different network.\")\n",
    "            print(\"Page source for debugging (first 2000 characters):\")\n",
    "            print(driver.page_source[:2000])\n",
    "            driver.quit()\n",
    "            return None, None\n",
    "\n",
    "        # Find the first profile in the profiles list and click it\n",
    "        try:\n",
    "            first_profile = WebDriverWait(driver, 30).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, \"//div[contains(@class, 'gs_ai_t')]//a\"))\n",
    "            )\n",
    "            profile_name = first_profile.text\n",
    "            profile_link = first_profile.get_attribute(\"href\")\n",
    "            print(f\"Found first profile: {profile_name} - URL: {profile_link}\")\n",
    "            driver.get(profile_link)  # Navigate to the profile page\n",
    "            time.sleep(3)\n",
    "            return profile_link, driver\n",
    "        except TimeoutException:\n",
    "            print(\"Timeout waiting for the first user profile.\")\n",
    "            print(\"Page source for debugging (first 2000 characters):\")\n",
    "            print(driver.page_source[:2000])\n",
    "            driver.quit()\n",
    "            return None, None\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding author profile: {e}\")\n",
    "        print(\"Page source for debugging (first 2000 characters):\")\n",
    "        print(driver.page_source[:2000])\n",
    "        driver.quit()\n",
    "        return None, None\n",
    "\n",
    "def scrape_publication_details(driver, publication_url):\n",
    "    try:\n",
    "        driver.get(publication_url)\n",
    "        time.sleep(3)  # Wait for the detail page to load\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Extract details from the publication page\n",
    "        title = soup.find('a', class_='gsc_a_at').text if soup.find('a', class_='gsc_a_at') else \"No title\"\n",
    "        \n",
    "        authors = \"N/A\"\n",
    "        journal = \"N/A\"\n",
    "        pub_date = \"N/A\"\n",
    "        volume = \"N/A\"\n",
    "        publisher = \"N/A\"\n",
    "        description = \"N/A\"\n",
    "        citations = \"N/A\"\n",
    "\n",
    "        # Extract metadata from the table (if present)\n",
    "        metadata_table = soup.find('div', id='gsc_oci_table')\n",
    "        if metadata_table:\n",
    "            rows = metadata_table.find_all('div', class_='gs_scl')\n",
    "            for row in rows:\n",
    "                field = row.find('div', class_='gsc_oci_field').text.lower()\n",
    "                value = row.find('div', class_='gsc_oci_value').text\n",
    "                if 'authors' in field:\n",
    "                    authors = value\n",
    "                elif 'journal' in field:\n",
    "                    journal = value\n",
    "                elif 'publication date' in field:\n",
    "                    pub_date = value\n",
    "                elif 'volume' in field:\n",
    "                    volume = value\n",
    "                elif 'publisher' in field:\n",
    "                    publisher = value\n",
    "\n",
    "        # Extract description (abstract)\n",
    "        description_elem = soup.find('div', id='gsc_oci_descr')\n",
    "        if description_elem:\n",
    "            description = description_elem.text.strip()\n",
    "\n",
    "        # Extract citations\n",
    "        citation_elem = soup.find('a', class_='gsc_oci_g_a')\n",
    "        if citation_elem:\n",
    "            citations_text = citation_elem.text\n",
    "            citations = re.sub(r'\\D', '', citations_text) if citations_text else \"0\"\n",
    "\n",
    "        return {\n",
    "            \"title\": title,\n",
    "            \"authors\": authors,\n",
    "            \"journal\": journal,\n",
    "            \"publication date\": pub_date,\n",
    "            \"total citation\": citations,\n",
    "            \"volume\": volume,\n",
    "            \"publisher\": publisher,\n",
    "            \"description\": description,\n",
    "            \"citations_per_year\": {}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping publication details from {publication_url}: {e}\")\n",
    "        return {\n",
    "            \"title\": \"Error\",\n",
    "            \"authors\": \"N/A\",\n",
    "            \"journal\": \"N/A\",\n",
    "            \"publication date\": \"N/A\",\n",
    "            \"total citation\": \"N/A\",\n",
    "            \"volume\": \"N/A\",\n",
    "            \"publisher\": \"N/A\",\n",
    "            \"description\": \"N/A\",\n",
    "            \"citations_per_year\": {}\n",
    "        }\n",
    "\n",
    "def scrape_publications(driver, max_publications=290):\n",
    "    try:\n",
    "        # Wait for the publications table to load\n",
    "        WebDriverWait(driver, 15).until(\n",
    "            EC.presence_of_element_located((By.ID, \"gsc_a_t\"))\n",
    "        )\n",
    "        print(\"Publications table loaded.\")\n",
    "\n",
    "        # Click \"Show more\" button repeatedly to load all publications\n",
    "        while True:\n",
    "            try:\n",
    "                publications = driver.find_elements(By.CLASS_NAME, \"gsc_a_tr\")\n",
    "                if len(publications) >= max_publications:\n",
    "                    print(f\"Reached maximum publications limit ({max_publications}).\")\n",
    "                    break\n",
    "                show_more_button = driver.find_element(By.ID, \"gsc_bpf_more\")\n",
    "                if show_more_button.get_attribute(\"disabled\"):\n",
    "                    print(\"No more publications to load.\")\n",
    "                    break\n",
    "                print(\"Clicking 'Show more' button...\")\n",
    "                show_more_button.click()\n",
    "                time.sleep(3)  # Wait for the next batch to load\n",
    "            except NoSuchElementException:\n",
    "                print(\"Show more button not found. All publications may already be loaded.\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error clicking 'Show more' button: {e}\")\n",
    "                break\n",
    "\n",
    "        # Extract publication links\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        publications = soup.find_all('tr', class_='gsc_a_tr')[:max_publications]\n",
    "        print(f\"Total publications to process: {len(publications)}\")\n",
    "\n",
    "        # Store the profile page URL to return to it after visiting each publication\n",
    "        profile_url = driver.current_url\n",
    "\n",
    "        data = []\n",
    "        all_years = set()\n",
    "\n",
    "        for i, pub in enumerate(publications, 1):\n",
    "            try:\n",
    "                title_elem = pub.find('a', class_='gsc_a_at')\n",
    "                title = title_elem.text\n",
    "                publication_url = f\"https://scholar.google.com{title_elem['href']}\"\n",
    "                print(f\"Processing publication {i}/{len(publications)}: {title}\")\n",
    "\n",
    "                # Scrape detailed information from the publication's page\n",
    "                pub_data = scrape_publication_details(driver, publication_url)\n",
    "\n",
    "                # Extract year for the year-wise citation columns\n",
    "                year_elem = pub.find('td', class_='gsc_a_y')\n",
    "                year = year_elem.text if year_elem else \"N/A\"\n",
    "                if year.isdigit():\n",
    "                    all_years.add(int(year))\n",
    "\n",
    "                data.append(pub_data)\n",
    "\n",
    "                # Return to the profile page\n",
    "                driver.get(profile_url)\n",
    "                time.sleep(3)  # Wait for the profile page to reload\n",
    "                WebDriverWait(driver, 15).until(\n",
    "                    EC.presence_of_element_located((By.ID, \"gsc_a_t\"))\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing publication {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        return data, all_years\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching publications: {e}\")\n",
    "        return [], set()\n",
    "\n",
    "def save_to_excel(publication_data, all_years, filename=\"author_details_output.xlsx\"):\n",
    "    final_data = []\n",
    "    for row in publication_data:\n",
    "        base_data = {\n",
    "            \"title\": row[\"title\"],\n",
    "            \"authors\": row[\"authors\"],\n",
    "            \"journal\": row[\"journal\"],\n",
    "            \"publication date\": row[\"publication date\"],\n",
    "            \"total citation\": row[\"total citation\"],\n",
    "            \"volume\": row[\"volume\"],\n",
    "            \"publisher\": row[\"publisher\"],\n",
    "        }\n",
    "        for year in all_years:\n",
    "            base_data[str(year)] = row.get(\"citations_per_year\", {}).get(year, 0)\n",
    "        base_data[\"description\"] = row[\"description\"]\n",
    "        final_data.append(base_data)\n",
    "    df = pd.DataFrame(final_data)\n",
    "    df.to_excel(filename, index=False)\n",
    "    print(f\"\\nData saved to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    author_name = input(\"Enter the author's name: \")\n",
    "    author_affiliation = input(\"Enter the author's affiliation (or keyword): \")\n",
    "\n",
    "    profile_url, driver = get_author_profile_url(author_name, author_affiliation)\n",
    "\n",
    "    if profile_url and driver:\n",
    "        try:\n",
    "            data, all_years = scrape_publications(driver)\n",
    "            if data:\n",
    "                save_to_excel(data, sorted(all_years))\n",
    "            else:\n",
    "                print(\"No publications found.\")\n",
    "        finally:\n",
    "            driver.quit()\n",
    "    else:\n",
    "        print(\"Author not found. Check the name or affiliation keyword.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bee538-20bc-40ee-9a19-6405223ca14a",
   "metadata": {},
   "source": [
    "## Article Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536ad2ea-8c12-4e24-9c4a-d2500ba78b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "def scrape_scholar_articles(query, num_pages):\n",
    "    articles = []\n",
    "    page = 0\n",
    "    while page < num_pages:\n",
    "        url = f\"https://scholar.google.com/scholar?start={page*10}&q={query}&hl=en&as_sdt=0,5\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        results = soup.find_all(\"div\", class_=\"gs_ri\")\n",
    "\n",
    "        for result in results:\n",
    "            title = result.find(\"h3\", class_=\"gs_rt\").text\n",
    "            authors = result.find(\"div\", class_=\"gs_a\").text\n",
    "            link = result.find(\"a\")[\"href\"]\n",
    "            articles.append({\"Title\": title, \"Authors\": authors, \"Link\": link})\n",
    "\n",
    "        page += 1\n",
    "\n",
    "    return articles\n",
    "\n",
    "def save_to_excel(articles, filename):\n",
    "    df = pd.DataFrame(articles)\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "def browse_folder():\n",
    "    folder_path = filedialog.askdirectory()\n",
    "    entry_folder.delete(0, tk.END)\n",
    "    entry_folder.insert(tk.END, folder_path)\n",
    "\n",
    "def scrape_articles():\n",
    "    query = entry_query.get()\n",
    "    num_pages = int(entry_pages.get())\n",
    "\n",
    "    articles = scrape_scholar_articles(query, num_pages)\n",
    "\n",
    "    folder_path = entry_folder.get()\n",
    "    if folder_path:\n",
    "        filename = f\"{folder_path}/scholar_articles.xlsx\"\n",
    "    else:\n",
    "        filename = \"scholar_articles.xlsx\"\n",
    "\n",
    "    save_to_excel(articles, filename)\n",
    "    label_status.config(text=\"Extraction complete. Data saved to scholar_articles.xlsx.\")\n",
    "\n",
    "# Create the main window\n",
    "window = tk.Tk()\n",
    "window.title(\"Google Scholar Scraper\")\n",
    "window.geometry(\"400x250\")\n",
    "\n",
    "# Create input fields and labels\n",
    "label_query = tk.Label(window, text=\"Article Title or Keyword:\")\n",
    "label_query.pack()\n",
    "entry_query = tk.Entry(window, width=40)\n",
    "entry_query.pack()\n",
    "\n",
    "label_pages = tk.Label(window, text=\"Number of Pages:\")\n",
    "label_pages.pack()\n",
    "entry_pages = tk.Entry(window, width=40)\n",
    "entry_pages.pack()\n",
    "\n",
    "label_folder = tk.Label(window, text=\"Output Folder (optional):\")\n",
    "label_folder.pack()\n",
    "entry_folder = tk.Entry(window, width=40)\n",
    "entry_folder.pack()\n",
    "\n",
    "# Create browse button\n",
    "button_browse = tk.Button(window, text=\"Browse\", command=browse_folder)\n",
    "button_browse.pack()\n",
    "\n",
    "# Create extract button\n",
    "button_extract = tk.Button(window, text=\"Extract Data\", command=scrape_articles)\n",
    "button_extract.pack()\n",
    "\n",
    "# Create status label\n",
    "label_status = tk.Label(window, text=\"\")\n",
    "label_status.pack()\n",
    "\n",
    "# Run the main window loop\n",
    "window.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
